{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Network Sourcing example\n",
    "Uses one production, two intermediate and multiple consumption locations.\n",
    "We're moving one product \"Beer\" measured in the dimension \"weight\"\n",
    "How are movements in the network costed?\n",
    "- The same costings as the basic and intermediate example are used:\n",
    "  - We have two lane rates between our main production center and the two warehouses\n",
    "  - and a distribution \"Cost Model\" between the sources (proudction + 2 x warehouses) and consumption nodes\n",
    "  - Lane rates between Production and Intermediate nodes are costed on a cost per km basis.\n",
    "  - Cost models to distribute the quantities further is also based on a (more expensive) cost per km.\n",
    "  - It's typical that high utilisation vehicles move between warehouses (and typically larger vehicles, achieving a lower cost per km / cost per ton)\n",
    "  - And that smaller vehicles handle the secondary distribution (at a higher cost per ton)\n",
    "  - adding a fixed cost trigger for using a warehouse\n",
    "  - this allows the model to select between the two warehouses, or potentially using both. \n",
    "- We're going to add a constraint where deliveries from the guiness storehouse may only be made withing a radius of 50 km's. This is typical when you need to restrict a fleet to a vehicle type (like a secondary delivery vehicle) which you don't want leaving a major city\n",
    "- we're also going to add a constraint where only 90% of the volume can be serviced by the production node, which means we'll incur consumption penalties, but that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv('../sample_data/publist_large.csv')\n",
    "# we'll just use the first 100 nodes for a free-tier key\n",
    "df = df.head(100) # free tier key, model too large\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('apiHelper.py').read()) # import some helper classes which we've written for you.\n",
    "\n",
    "api = apiHelper(modelType=\"ns3-tbfvuwtge2iq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets instantiate a model container so that we can build out a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = ns3_tbfvuwtge2iq_pb2.SolveRequest()\n",
    "sr.geometryOutput = 1  # set the geometry output to aggregate\n",
    "sr.solveType = 0       # optimise solve request.\n",
    "m = sr.model\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the model helper which builds some common objects used in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('ns3-model-helper.py').read()) # import some modelling helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model outline\n",
    "* this model is going to define distance, time and weight (the dimensions).\n",
    "* we'll create a single production node, two warehouse nodes, and the balance as demand (or consumption) nodes.\n",
    "* we'll add lane rates between the production nodes and the warehouses, and costmodels for the warehouses (which will automatically connect with the demand nodes)\n",
    "* This model will use a single product (\"Beer\") to illustrate a minimal model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dimensions.CopyFrom(make_distance_time_user_dimensions('weight'))\n",
    "m.dimensions # we have time, distance and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to split the data into three node components. \n",
    "1. A product node where the demand is -1\n",
    "2. Warehouse nodes where the demand is -2\n",
    "3. All other nodes where the demand > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "productionNodes = df[df['demand'] == -1]\n",
    "warehouseNodes =  df[df['demand'] == -2]\n",
    "demandNodes = df[df['demand'] > 0]\n",
    "print(productionNodes)\n",
    "print('-------------')\n",
    "print(warehouseNodes)\n",
    "print('-------------')\n",
    "print(demandNodes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now convert all these data rows into nodes which we can use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nodes = make_nodes(productionNodes)\n",
    "w_nodes = make_nodes(warehouseNodes)\n",
    "d_nodes = make_nodes(demandNodes)\n",
    "print(p_nodes)\n",
    "print('-------------')\n",
    "print(w_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Cost Triggers\n",
    "We're going to add a fixed cost trigger to each of the warehouses. This means that if any non-zero quantity flows through a warehouse, the full fixed cost of using that warehouse is incurred. This is useful for modelling contracts that are taken out on an annualised basis or minimum staffing levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, e in enumerate(w_nodes):\n",
    "    f = ns3_tbfvuwtge2iq_pb2.Node.Flow()\n",
    "    fc = ns3_tbfvuwtge2iq_pb2.FixedDimensionCost()\n",
    "    fc.dimensionIds.append('weight')\n",
    "    fc.fixedCost = 10000\n",
    "    f.FixedDimensionCosts.append(fc)\n",
    "    e.flow.CopyFrom(f)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to define which nodes require a certain amount of each dimension (consumption nodes), followed by the nodes where volumes can be produced (production)\n",
    "\n",
    "### Limiting the sources based on distance\n",
    "In this example, we're going to limit the nodes which may be serviced by the Guiness Storehouse. Only locations within 50km's, as the crow flies on a curved earth (sorry flat earthers...), may be serviced from the Dublin facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in enumerate(d_nodes):\n",
    "    pf = ns3_tbfvuwtge2iq_pb2.Node.ProductFlow()\n",
    "    pf.productId = 'Beer'\n",
    "    # each demand node must have the quantity demand[i] delivered, so the range here\n",
    "    # is actually [demand[i], demand[i]]. \n",
    "    # Not meeting this range incurs a large penalty cost.\n",
    "    di = demandNodes.iloc[i]['demand']\n",
    "    pf.dimensionRanges.append(make_dimension_range(\"weight\", di, di))\n",
    "    del d_nodes[i].consumption[:] #in case you run this twice\n",
    "    d_nodes[i].consumption.append(pf)\n",
    "    \n",
    "    del d_nodes[i].allowableSources[:] # in case you run this twice\n",
    "    sources = list(warehouseNodes['id'])\n",
    "    if great_circle(demandNodes.iloc[i]['longitude'], demandNodes.iloc[i]['latitude'],\n",
    "             p_nodes[0].geocode.longitude, p_nodes[0].geocode.latitude) < 50:\n",
    "        sources.append(p_nodes[0].id)\n",
    "        \n",
    "    d_nodes[i].allowableSources.extend(sources) \n",
    "    d_nodes[i].maximumSources = 1 # we must select exactly one\n",
    "\n",
    "print(d_nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in enumerate(p_nodes):\n",
    "    pf = ns3_tbfvuwtge2iq_pb2.Node.ProductFlow()\n",
    "    pf.productId = 'Beer'\n",
    "    # We're going to limit production to 90% of the total so that we pick the cheapest\n",
    "    # subset of demand which gets us close to that 90% of the total at minimum cost.\n",
    "    pf.dimensionRanges.append(make_dimension_range(\"weight\", 0, 0.9 * sum(demandNodes['demand'])))\n",
    "    pf.dimensionRanges[0].flowPenalty = 1e8\n",
    "    del p_nodes[i].production[:] #in case you run this twice\n",
    "    p_nodes[i].production.append(pf)\n",
    "    \n",
    "print(p_nodes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the node definitions are complete, we can add them all to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m.nodes[:]\n",
    "m.nodes.extend(p_nodes)\n",
    "m.nodes.extend(w_nodes)\n",
    "m.nodes.extend(d_nodes)\n",
    "print(len(m.nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to define some lane rates between the main nodes in the network.\n",
    "1. Guiness Storehouse -> Limerick\n",
    "2. and Guinnes Storehouse -> Galway\n",
    "\n",
    "each costed at 0.1 monetary units per km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sources = list(df[df['demand'] < 0]['id'])\n",
    "del m.laneRates[:]\n",
    "m.laneRates.append(make_lane_rate_distance(sources[0], sources[1], 0.1))\n",
    "m.laneRates.append(make_lane_rate_distance(sources[0], sources[2], 0.1))\n",
    "print(m.laneRates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the product group we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m.productGroups[:]\n",
    "m.productGroups.append(make_single_product_group(\"Beer\", \"weight\"))\n",
    "print(m.productGroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we're going to define the cost models for the secondary distribution from each of the sources to the delivery footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m.costModels[:]\n",
    "for index, i in enumerate(sources):\n",
    "    m.costModels.append(make_cost_model_distance(i, 0.2))\n",
    "\n",
    "print(m.costModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the model\n",
    "We now have a complete model specified so we can submit it to the api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqId = api.Post(sr)\n",
    "print(reqId) # if you want to see what the guid looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the model response\n",
    "We can use the api-helper to grab the solution response using the request-id provided by the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = api.Get(reqId)\n",
    "\n",
    "print(sol.objective) # This is the cost of the solution returned by the api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the response\n",
    "The network sourcing model returns item which can be easily tabulated. For this reason, we've wrapped a couple of common functions in the apiHelper.py file which can be used to tabulate the response. \n",
    "The geometries are returned in a special format in this model which are indexed by their common overlapping sections (so that the response payload is smaller). \n",
    "\n",
    "There are three main tables, the assignments (i.e. which lane rates or cost models are selected), the node flow, and node product flow (which indicate the flow _over_ a node, either in the aggregate or by product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tabulate(sr, sol)\n",
    "print (\"--------- assignments --------- \")\n",
    "print (tab['assignments'].head())\n",
    "print (\"\\n--------- node flows --------- \")\n",
    "print (tab['nodeFlows'].head())\n",
    "print (\"\\n--------- node product flows --------- \")\n",
    "print (tab['nodeProductFlows'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab['routes'].head() # and these are the geometries between each of the locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the response\n",
    "\n",
    "Lets go ahead and visualise the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Polyline, Circle, LayerGroup\n",
    "\n",
    "# add some locations to the map (just black dots should be ok)\n",
    "locs = list()\n",
    "for index, p in tab['nodes'].iterrows():\n",
    "    circle = Circle()\n",
    "    circle.location = (p['y'], p['x'])\n",
    "    circle.radius = 10\n",
    "    circle.color = \"black\"\n",
    "    locs.append(circle)\n",
    "\n",
    "center = [df['latitude'][1],df['longitude'][1]] # just use the first point as the center (i.e. the guiness storehouse)\n",
    "mp = Map(scroll_wheel_zoom=True, center=center, zoom=6)\n",
    "for i, gs in enumerate(tab['routes']['geometry']):\n",
    "    mp.add_layer(Polyline(locations=gs,color='blue', fill=False))\n",
    "\n",
    "mp.add_layer(LayerGroup(layers=(locs)))\n",
    "mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again quite interesting. \n",
    "We're essentally opening two warehouses now and moving product back through the network to get to the outskirts of Dublin (which we expected by construction). We're also using both warehouses now. If the fixed cost was higher, we may find that we would go back to selecting just one. Clearly we should explore a warehouse in Dublin that could reach more of the surrounding area at a lower cost. But this is simply modelling details now. Note that some nodes go unserviced - this is because we don't have production capacity anymore to service all nodes.\n",
    "\n",
    "We can verify this by looking the node flow (or product node flow) through the different warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab['nodeFlows'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view unserviced nodes, we can just filter where there is a consumption penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab['nodeFlows'][tab['nodeFlows']['consumptionPenalty'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "It's time to play around with your own data! Remember that if you have any question feel free to reach out to us and we'll see how we can help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
